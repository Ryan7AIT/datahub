{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOn8aFdulhoRgK7Y8OpMX3H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ryan7AIT/datahub/blob/main/rul.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Oxr0B8l-lxw",
        "outputId": "48eff35f-d898-4d62-928e-36fa90d37f32"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=de679d1680be8aa5e0d9b5a5b090b38a6485eaa24ae06df9c87a327921bb7ee1\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJhGfuy--1SW",
        "outputId": "59721ae9-3b88-4d44-8d78-151e5d740985"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "id": "jmvb-qO49zIU",
        "outputId": "3252d235-6c56-47c7-d794-83b312564833"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/subprocess.py:1796: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = _posixsubprocess.fork_exec(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/Users/mac/Downloads/rul4.csv.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-66451807e8c0>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Load your dataset into a PySpark DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/mac/Downloads/rul4.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# df = df.limit(1000)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/Users/mac/Downloads/rul4.csv."
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, udf, to_date, dayofweek, hour, lag, mean, lit, when,avg\n",
        "from pyspark.sql.window import Window\n",
        "import json\n",
        "from pyspark.sql.functions import rand, isnan, when\n",
        "from pyspark.sql.types import FloatType\n",
        "from pyspark.sql.functions import rand, round\n",
        "from keras.callbacks import TensorBoard\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import when, col, monotonically_increasing_id\n",
        "from pyspark.sql.types import FloatType\n",
        "import math\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "import datetime\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Initialize a SparkSession\n",
        "spark = SparkSession.builder.appName(\"PredictiveMaintenance\").getOrCreate()\n",
        "\n",
        "# Load your dataset into a PySpark DataFrame\n",
        "df = spark.read.csv('/Users/mac/Downloads/rul4.csv', header=True, inferSchema=True)\n",
        "df.count()\n",
        "# df = df.limit(1000)\n",
        "df.count()\n",
        "# Assuming 'date_insertion' is the correct timestamp column based on your dataset schema\n",
        "# Adjust the following transformations accordingly:\n",
        "\n",
        "# Extract JSON fields function\n",
        "def extract_from_json(column, key):\n",
        "    try:\n",
        "        json_data = json.loads(column.replace(\"'\", \"\\\"\"))\n",
        "        return json_data.get(key, None)\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "# Registering the UDF\n",
        "extract_from_json_udf = udf(extract_from_json)\n",
        "\n",
        "# Step 1: Extracting 'oil_value' and 'fuel_liters'\n",
        "df = df.withColumn(\"oil_value\", extract_from_json_udf(col(\"details\"), lit(\"oil_value\")))\n",
        "df = df.withColumn(\"fuel_liters\", extract_from_json_udf(col(\"details\"), lit(\"fuel_liters\")))\n",
        "\n",
        "# Step 2: Creating time-based features\n",
        "# df = df.withColumn(\"date_insertion\", to_date(col(\"date_insertion\")))\n",
        "df = df.withColumn(\"day_of_week\", dayofweek(col(\"date_insertion\")))\n",
        "df = df.withColumn(\"hour_of_day\", hour(col(\"date_insertion\")))\n",
        "\n",
        "\n",
        "\n",
        "# Step 4: Aggregate readings on a daily basis\n",
        "daily_avg_df = df.groupBy(\"thing_id\", \"date_insertion\").agg(mean(\"power_supply_voltage\").alias(\"daily_avg_voltage\"))\n",
        "df = df.join(daily_avg_df, [\"thing_id\", \"date_insertion\"], \"left\")\n",
        "\n",
        "# Step 5: Create binary indicator for 'engine_status'\n",
        "df = df.withColumn(\"engine_alert\", when(col(\"engine_status\") == \"Abnormal\", 1).otherwise(0))\n",
        "\n",
        "# Define a UDF to generate random values within a range\n",
        "def random_value(min_value, max_value):\n",
        "    return (rand() * (max_value - min_value) + min_value).cast(FloatType())\n",
        "\n",
        "# random_value_udf = udf(random_value, FloatType())\n",
        "\n",
        "# Set min and max values for 'oil_value' and 'fuel_liters'\n",
        "oil_value_min, oil_value_max = 0, 4\n",
        "fuel_liters_min, fuel_liters_max = 0, 60\n",
        "\n",
        "fuel_liters_mi, fuel_liters_ma = 2, 6\n",
        "\n",
        "\n",
        "# Replace null values with random numbers\n",
        "# Replace null values with random numbers and round to 1 decimal place\n",
        "# df = df.withColumn(\"oil_value\", when(df['oil_value'].isNull(), round((rand() * (oil_value_max - oil_value_min) + oil_value_min), 1)).otherwise(df['oil_value']))\n",
        "\n",
        "\n",
        "# Sample range for oil values\n",
        "oil_value_min = 0.5\n",
        "oil_value_max = 5.0\n",
        "\n",
        "df = df.orderBy('thing_id', 'date_insertion')\n",
        "\n",
        "from pyspark.sql.functions import row_number\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "window = Window.partitionBy('thing_id').orderBy('date_insertion')\n",
        "df = df.withColumn('row_num', row_number().over(window))\n",
        "\n",
        "from pyspark.sql.functions import count\n",
        "\n",
        "window = Window.partitionBy('thing_id')\n",
        "df = df.withColumn('total_rows', count('*').over(window))\n",
        "\n",
        "from pyspark.sql.functions import when, rand\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Find the maximum total_km value\n",
        "max_total_km = df.agg(max(col(\"total_km\"))).collect()[0][0]\n",
        "\n",
        "\n",
        "# Define a window specification to rank rows with the same total_km\n",
        "window_spec = Window.partitionBy(\"total_km\").orderBy(\"total_km\")\n",
        "\n",
        "# Add a row number column to the DataFrame\n",
        "df_with_row_number = df.withColumn(\"row_number\", row_number().over(window_spec))\n",
        "\n",
        "# Filter rows to keep only one with the maximum total_km value\n",
        "filtered_max_total_km = df_with_row_number.filter((col(\"total_km\") == max_total_km) & (col(\"row_number\") == 1)).drop(\"row_number\")\n",
        "\n",
        "# Filter out all rows with the maximum total_km except one\n",
        "remaining_data = df.filter(col(\"total_km\") != max_total_km)\n",
        "\n",
        "# Union the remaining data with the single row with the maximum total_km\n",
        "df = remaining_data.union(filtered_max_total_km)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from pyspark.sql.functions import when, rand, round, col\n",
        "df = df.withColumn(\"oil_value\", col(\"oil_value\").cast(\"float\"))\n",
        "# Define the ranges for the oil values\n",
        "# Define the ranges for the oil values\n",
        "first_range_min = 4\n",
        "first_range_max = 6\n",
        "second_range_min = 3\n",
        "second_range_max = 6\n",
        "third_range_min = 2\n",
        "third_range_max = 5\n",
        "fourth_range_min = 1.5\n",
        "fourth_range_max = 3\n",
        "\n",
        "\n",
        "first_range_min2 = 0.014\n",
        "first_range_max2 = 0.017\n",
        "second_range_min2 = 0.014\n",
        "second_range_max2 = 0.018\n",
        "third_range_min2 = 0.015\n",
        "third_range_max2 = 0.018\n",
        "fourth_range_min2 = 0.017\n",
        "fourth_range_max2 = 0.022\n",
        "\n",
        "\n",
        "\n",
        "# Define a window partitioned by 'thing_id' and ordered by 'date_insertion'\n",
        "window = Window.partitionBy('thing_id').orderBy('date_insertion')\n",
        "total_window = Window.partitionBy('thing_id')\n",
        "\n",
        "# Calculate the row number and total rows for each 'thing_id'\n",
        "df = df.withColumn('row_num', row_number().over(window))\n",
        "df = df.withColumn('total_rows', count('*').over(total_window))\n",
        "\n",
        "# Define the thresholds for the different periods\n",
        "df = df.withColumn('first_threshold', (col('total_rows') * 0.6))\n",
        "df = df.withColumn('second_threshold', (col('total_rows') * 0.8))\n",
        "df = df.withColumn('third_threshold', (col('total_rows') * 0.9))\n",
        "\n",
        "# Generate the oil values\n",
        "df = df.withColumn('oil_value',\n",
        "                   when(df['oil_value'].isNull() & (col('row_num') <= col('first_threshold')),\n",
        "                        round(rand() * (first_range_max - first_range_min) + first_range_min, 1))\n",
        "                   .when(df['oil_value'].isNull() & (col('row_num') > col('first_threshold')) & (col('row_num') <= col('second_threshold')),\n",
        "                         round(rand() * (second_range_max - second_range_min) + second_range_min, 1))\n",
        "                   .when(df['oil_value'].isNull() & (col('row_num') > col('second_threshold')) & (col('row_num') <= col('third_threshold')),\n",
        "                         round(rand() * (third_range_max - third_range_min) + third_range_min, 1))\n",
        "                   .when(df['oil_value'].isNull() & (col('row_num') > col('third_threshold')),\n",
        "                         round(rand() * (fourth_range_max - fourth_range_min) + fourth_range_min, 1))\n",
        "                   .otherwise(df['oil_value']))\n",
        "\n",
        "\n",
        "# create an empty fuel change colunm\n",
        "df = df.withColumn('fuel_change', lit(None).cast(FloatType()))\n",
        "\n",
        "# generate rhe fule change values\n",
        "df = df.withColumn('fuel_change',\n",
        "                   when(df['fuel_change'].isNull() & (col('row_num') <= col('first_threshold')),\n",
        "                        round(rand() * (first_range_max2 - first_range_min2) + first_range_min2, 3))\n",
        "                   .when(df['fuel_change'].isNull() & (col('row_num') > col('first_threshold')) & (col('row_num') <= col('second_threshold')),\n",
        "                         round(rand() * (second_range_max2 - second_range_min2) + second_range_min2, 3))\n",
        "                   .when(df['fuel_change'].isNull() & (col('row_num') > col('second_threshold')) & (col('row_num') <= col('third_threshold')),\n",
        "                         round(rand() * (third_range_max2 - third_range_min2) + third_range_min2, 3))\n",
        "                   .when(df['fuel_change'].isNull() & (col('row_num') > col('third_threshold')),\n",
        "                         round(rand() * (fourth_range_max2 - fourth_range_min2) + fourth_range_min2, 3))\n",
        "                   .otherwise(df['fuel_change']))\n",
        "\n",
        "# Drop the auxiliary columns\n",
        "df = df.drop('row_num', 'total_rows', 'first_threshold', 'second_threshold', 'third_threshold')\n",
        "\n",
        "\n",
        "# =====================================\n",
        "# =====================================\n",
        "# =====================================\n",
        "# =====================================\n",
        "# =====================================\n",
        "# =====================================\n",
        "# =====================================\n",
        "# fuel change\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "df = df.withColumn(\"fuel_liters\", when(df['fuel_liters'].isNull(), round((rand() * (fuel_liters_max - fuel_liters_min) + fuel_liters_min), 1)).otherwise(df['fuel_liters']))\n",
        "# df = df.withColumn(\"fuel_change\", when(df['fuel_liters'].isNull(), round((rand() * (fuel_liters_ma - fuel_liters_mi) + fuel_liters_mi), 1)).otherwise(df['fuel_liters']))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Step 6: Generate interaction features\n",
        "# df = df.withColumn(\"voltage_current_interaction\", col(\"power_supply_voltage\") * col(\"battery_current\"))\n",
        "\n",
        "# Step 3: Calculating rate of change for 'battery_current'\n",
        "windowSpec = Window.partitionBy(\"thing_id\").orderBy(\"date_insertion\")\n",
        "df = df.withColumn(\"battery_current_change\", col(\"power_supply_voltage\") - lag(\"power_supply_voltage\", 1).over(windowSpec))\n",
        "\n",
        "\n",
        "df = df.select(\"thing_id\", \"date_insertion\", \"speed\", \"total_km\", \"engine_status\", \"power_supply_voltage\" ,\"oil_value\",\"fuel_change\", \"fuel_liters\",  \"battery_current_change\", \"daily_avg_voltage\")\n",
        "\n",
        "\n",
        "from pyspark.sql import Window\n",
        "from pyspark.sql.functions import lag, avg, stddev\n",
        "\n",
        "# Define a window\n",
        "window = Window.orderBy('date_insertion').rowsBetween(-9, 0)  # assuming 'date_insertion' is your time column\n",
        "\n",
        "# Calculate rolling averages and standard deviations\n",
        "# df = df.withColumn('speed_avg', avg(df['speed']).over(window))\n",
        "# df = df.withColumn('oil_value_std', stddev(df['oil_value']).over(window))\n",
        "\n",
        "# Calculate changes between consecutive readings\n",
        "# df = df.withColumn('speed_change', df['speed'] - lag(df['speed']).over(Window.orderBy('date_insertion')))\n",
        "# df = df.withColumn('fuel_change', df['fuel_liters'] - lag(df['fuel_liters']).over(Window.orderBy('date_insertion')))\n",
        "# df = df.withColumn('fuel_change', (rand() * 4 + 2).cast(\"int\"))#replace all fuel_chnages values with random values between 2 and 6\n",
        "\n",
        "\n",
        "\n",
        "# Step 7: Calculate rolling average\n",
        "\n",
        "\n",
        "# Define a Window specification\n",
        "# windowSpec = Window.orderBy('date_insertion').rowsBetween(-4, 0)  # 5 rows including current row\n",
        "\n",
        "# Calculate rolling average\n",
        "# df = df.withColumn('oil_quality_rolling_avg', avg(df['oil_value']).over(windowSpec))\n",
        "\n",
        "df = df.withColumn(\"car_age\", when(rand() < 0.7, \"old\").otherwise(\"new\"))\n",
        "\n",
        "# Show the first 5 rows of the DataFrame\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dO4TCx8y-kRa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}